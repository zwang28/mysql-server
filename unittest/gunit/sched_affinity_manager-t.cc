/*****************************************************************************
Copyright (c) 2020, Huawei Technologies Co., Ltd. All Rights Reserved.
This program is free software; you can redistribute it and/or modify it under
the terms of the GNU General Public License, version 2.0, as published by the
Free Software Foundation.
This program is distributed in the hope that it will be useful, but WITHOUT
ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE. See the GNU General Public License, version 2.0,
for more details.
*****************************************************************************/

#include "sql/sched_affinity_manager.h"
#ifdef HAVE_LIBNUMA
#include <numa.h>
#endif
#include <string>
#include <iostream>
#include <thread>
#include <limits>
#include <algorithm>
#include <numeric>
#include <atomic>
#include <mutex>
#include <sys/syscall.h>

#include "gtest/gtest.h"
#include "my_config.h"

#ifdef HAVE_LIBNUMA

using ::sched_affinity::Sched_affinity_manager;
using ::sched_affinity::Sched_affinity_manager_numa;
using ::sched_affinity::Thread_type;
using ::testing::TestInfo;
using namespace std;

namespace sched_affinity {

const Thread_type thread_types[] = {
    Thread_type::FOREGROUND,         Thread_type::LOG_WRITER,
    Thread_type::LOG_FLUSHER,        Thread_type::LOG_WRITE_NOTIFIER,
    Thread_type::LOG_FLUSH_NOTIFIER, Thread_type::LOG_CLOSER,
    Thread_type::LOG_CHECKPOINTER,   Thread_type::PURGE_COORDINATOR};

class SchedAffinityManagerTest : public ::testing::Test {
 public:
  static bitmask *INVALID_BITMASK_PTR;

  static pid_t gettid()
  {
      return static_cast<pid_t>(syscall(SYS_gettid));
  }
  
  static bool invoke_init_sched_affinity_info(
      Sched_affinity_manager_numa &instance, const std::string &cpu_string,
      bitmask *&group_bitmask) {
    return instance.init_sched_affinity_info(cpu_string, group_bitmask);
  }

  static bool invoke_init_sched_affinity_group(
      Sched_affinity_manager_numa &instance, const bitmask *group_bitmask,
      const bool numa_aware,
      std::vector<Sched_affinity_group> &sched_affinity_group) {
    return instance.init_sched_affinity_group(group_bitmask, numa_aware,
                                              sched_affinity_group);
  }

  static std::pair<std::string, bool> invoke_normalize_cpu_string(
      const std::string &cpu_string) {
    return Sched_affinity_manager_numa::normalize_cpu_string(cpu_string);
  }

  static bool invoke_is_thread_sched_enabled(
      Sched_affinity_manager_numa &instance, const Thread_type thread_type) {
    return instance.is_thread_sched_enabled(thread_type);
  }

  static bool invoke_bind_to_group(Sched_affinity_manager_numa &instance, const pid_t pid) {
    return instance.bind_to_group(pid);
  }

  static bool invoke_unbind_from_group(Sched_affinity_manager_numa &instance, const pid_t pid) {
    return instance.unbind_from_group(pid);
  }

  static Thread_type invoke_get_thread_type_by_pid(Sched_affinity_manager_numa &instance, const pid_t pid) {
    return instance.get_thread_type_by_pid(pid);
  }

  static std::map<Thread_type, std::set<pid_t>>& get_m_thread_pid(Sched_affinity_manager_numa &instance) {
    return instance.m_thread_pid;
  }

  static std::map<Thread_type, std::vector<Sched_affinity_group>>
      &get_m_sched_affinity_group(Sched_affinity_manager_numa &instance) {
    return instance.m_sched_affinity_group;
  }

  static std::map<pid_t, int> &get_m_pid_group_id(
      Sched_affinity_manager_numa &instance) {
    return instance.m_pid_group_id;
  }

  template<typename F, typename ...V>
  auto invoke_func(F func, V... args)
  {
    return (*func)(args...);
  }

  template<typename T, typename F, typename ...V>
  auto invoke_member_func(T& instance,F func, V... args)
  {
    return (instance.*func)(args...);
  }

  void start_threads(int thread_number, std::vector<std::thread> &threads,
                     std::vector<pid_t> &thread_pids,
                     std::atomic<bool> &is_stopped) {
    using namespace std::chrono_literals;
    std::mutex mutex;
    is_stopped.store(false);
    threads.clear();
    for (auto i = 0; i < thread_number; ++i) {
      threads.push_back(std::thread([&mutex, &thread_pids, &is_stopped]() {
        auto pid = gettid();
        {
          std::lock_guard<std::mutex> gurad(mutex);
          thread_pids.push_back(pid);
        }
        while (!is_stopped.load()) {
          std::this_thread::sleep_for(100ms);
        }
      }));
    }

    while (true) {
      {
        std::lock_guard<std::mutex> gurad(mutex);
        if (static_cast<int>(thread_pids.size()) == thread_number) {
          break;
        }
      }
      std::this_thread::sleep_for(100ms);
    }
  }

 protected:
  bool skip_if_numa_unavailable() {
    if (numa_available() == -1) {
      SUCCEED() << "Skip test case as numa is unavailable.";
      return true;
    } else {
      return false;
    }
  }

  std::string convert_bitmask_to_string(struct bitmask* bitmask, int& min, int& max) {
    std::string res;

    int i = 0, total_cpu_num = numa_num_configured_cpus();
    bool flag1 = false, flag2 = false;
    for (; i < total_cpu_num; ) {
      int start = i, curr = i;

      while (curr < total_cpu_num 
             && numa_bitmask_isbitset(bitmask, curr)) {
        max = curr;
        curr++;
        flag2 = true;
      }

      if(flag1 && flag2) {
        res += ",";
      }

      if(curr != start && !flag1) {
        flag1 = true;
        min = start;
      }

      if (curr != start && curr - start == 1) {
        res += std::to_string(start);
      } else if (curr != start) {
        res += std::to_string(start) + "-" + std::to_string(curr-1);
      }
      if (curr == start) {
        i++;
      } else {
        i += (curr - start);
      }

      flag2 = false;
    }

    return res;
  }

  bool check_bind_to_target(int index) {
    struct bitmask *sched_bitmask = numa_allocate_cpumask();
    int ret = numa_sched_getaffinity(0, sched_bitmask);

    if (ret != -1) {
      bool flag = false;
      for (int i = 0; i < numa_num_configured_cpus(); i++) {
        if ((i == index && !numa_bitmask_isbitset(sched_bitmask, i)) 
            || (i != index && numa_bitmask_isbitset(sched_bitmask, i))) {
          flag = true;
          break;
        }
      }
      if (flag) {
        return false;
      }
    } else {
      return false;
    }

    numa_free_cpumask(sched_bitmask);
    sched_bitmask = nullptr;
    return true;
  }

  int get_avail_node_num(int node_num) {
    int res = 0;
    struct bitmask *bm = numa_get_run_node_mask();

    for (int i = 0; i < node_num; i++) {
      if (numa_bitmask_isbitset(bm, i)) {
        res++;
      }
    }

    return res;
  }

  void init_avail_cpu_num_per_node(std::vector<int> &avail_cpu_num_per_node, int cpu_num_per_node, 
                                   struct bitmask *default_bitmask) {
    for (int i = 0; i < test_process_node_num; i++) {
      int count = 0;
      for (int j = i * cpu_num_per_node; j < (i + 1) * cpu_num_per_node; j++) {
        if (numa_bitmask_isbitset(default_bitmask, j)) {
          count++;
        }
      }
      avail_cpu_num_per_node[i] = count;
    }
  }

  void init_avail_nodes_arr(std::vector<char> &avail_nodes_arr, int test_process_node_num) {
    struct bitmask *test_process_node_mask = numa_get_run_node_mask();

    for (int i = 0, arr_index = 0; i < test_process_node_num; i++) {
      if (numa_bitmask_isbitset(test_process_node_mask, i)) {
        avail_nodes_arr[arr_index++] = i;
      }
    }
  }

 protected:
  void SetUp() {
    if (skip_if_numa_unavailable()) {
      return;
    } else {
      test_process_cpu_num = numa_num_configured_cpus();
      test_process_node_num = numa_num_configured_nodes();

      ASSERT_GT(test_process_cpu_num, 0);
      ASSERT_GT(test_process_node_num, 0);

      cpu_num_per_node = test_process_cpu_num / test_process_node_num;
      test_avail_node_num = get_avail_node_num(test_process_node_num);

      
      default_bitmask = numa_allocate_cpumask();
      int ret = numa_sched_getaffinity(0, default_bitmask);
      ASSERT_NE(ret, -1);

      cpu_range_str = convert_bitmask_to_string(default_bitmask, cpu_range_min, cpu_range_max);

      avail_nodes_arr.resize(test_avail_node_num);
      avail_cpu_num_per_node.resize(test_process_node_num);

      init_avail_nodes_arr(avail_nodes_arr, test_process_node_num);
      init_avail_cpu_num_per_node(avail_cpu_num_per_node, cpu_num_per_node, default_bitmask);

      default_config = {
        {sched_affinity::Thread_type::FOREGROUND, nullptr},
        {sched_affinity::Thread_type::LOG_WRITER, nullptr},
        {sched_affinity::Thread_type::LOG_FLUSHER, nullptr},
        {sched_affinity::Thread_type::LOG_WRITE_NOTIFIER, nullptr},
        {sched_affinity::Thread_type::LOG_FLUSH_NOTIFIER, nullptr},
        {sched_affinity::Thread_type::LOG_CLOSER, nullptr},
        {sched_affinity::Thread_type::LOG_CHECKPOINTER, nullptr},
        {sched_affinity::Thread_type::PURGE_COORDINATOR, nullptr}};
    }
  }

  void TearDown() {
    numa_free_cpumask(default_bitmask);
    default_bitmask = nullptr;
  }

 protected:
  int test_process_cpu_num;

  int test_process_node_num;

  int cpu_num_per_node;

  int test_avail_node_num;

  int cpu_range_min;

  int cpu_range_max;

  std::vector<char> avail_nodes_arr;

  std::vector<int> avail_cpu_num_per_node;

  std::string cpu_range_str;

  const int BUFFER_SIZE_1024 = 1024;

  std::map<Thread_type, const char *> default_config;

  struct bitmask *default_bitmask;
};

bitmask* SchedAffinityManagerTest::INVALID_BITMASK_PTR = reinterpret_cast<bitmask*>(std::numeric_limits<unsigned long long>::max());

TEST_F(SchedAffinityManagerTest, AllNullptrConfig) {
  if (skip_if_numa_unavailable()) {
    return;
  }

  std::map<sched_affinity::Thread_type, const char *> all_nullptr_config = {
      {sched_affinity::Thread_type::FOREGROUND, nullptr},
      {sched_affinity::Thread_type::LOG_WRITER, nullptr},
      {sched_affinity::Thread_type::LOG_FLUSHER, nullptr},
      {sched_affinity::Thread_type::LOG_WRITE_NOTIFIER, nullptr},
      {sched_affinity::Thread_type::LOG_FLUSH_NOTIFIER, nullptr},
      {sched_affinity::Thread_type::LOG_CLOSER, nullptr},
      {sched_affinity::Thread_type::LOG_CHECKPOINTER, nullptr},
      {sched_affinity::Thread_type::PURGE_COORDINATOR, nullptr}};

  auto instance =
      Sched_affinity_manager::create_instance(all_nullptr_config, false);
  ASSERT_NE(instance, nullptr);
  ASSERT_EQ(typeid(*instance), typeid(Sched_affinity_manager_numa));
  for (auto thread_type : thread_types) {
    ASSERT_FALSE(invoke_is_thread_sched_enabled(
        *dynamic_cast<Sched_affinity_manager_numa *>(instance), thread_type));
  }

  Sched_affinity_manager::free_instance();
}

TEST_F(SchedAffinityManagerTest, EmptyStringConfig) {
  if (skip_if_numa_unavailable()) {
    return;
  }

  std::map<sched_affinity::Thread_type, const char *> empty_string_config = {
      {sched_affinity::Thread_type::FOREGROUND, ""},
      {sched_affinity::Thread_type::LOG_WRITER, ""},
      {sched_affinity::Thread_type::LOG_FLUSHER, ""},
      {sched_affinity::Thread_type::LOG_WRITE_NOTIFIER, ""},
      {sched_affinity::Thread_type::LOG_FLUSH_NOTIFIER, ""},
      {sched_affinity::Thread_type::LOG_CLOSER, ""},
      {sched_affinity::Thread_type::LOG_CHECKPOINTER, ""},
      {sched_affinity::Thread_type::PURGE_COORDINATOR, ""}};

  auto instance =
      Sched_affinity_manager::create_instance(empty_string_config, false);
  ASSERT_NE(instance, nullptr);
  ASSERT_EQ(typeid(*instance), typeid(Sched_affinity_manager_numa));
  for (auto thread_type : thread_types) {
    ASSERT_FALSE(invoke_is_thread_sched_enabled(
        *dynamic_cast<Sched_affinity_manager_numa *>(instance), thread_type));
  }

  Sched_affinity_manager::free_instance();
}

TEST_F(SchedAffinityManagerTest, EmptyContainerConfig) {
  if (skip_if_numa_unavailable()) {
    return;
  }

  std::map<sched_affinity::Thread_type, const char *> empty_container_config =
      {};

  auto instance =
      Sched_affinity_manager::create_instance(empty_container_config, false);
  ASSERT_NE(instance, nullptr);
  ASSERT_EQ(typeid(*instance), typeid(Sched_affinity_manager_numa));
  for (auto thread_type : thread_types) {
    ASSERT_FALSE(invoke_is_thread_sched_enabled(
        *dynamic_cast<Sched_affinity_manager_numa *>(instance), thread_type));
  }

  Sched_affinity_manager::free_instance();
}

TEST_F(SchedAffinityManagerTest, NormalizeCpuString) {
  if (skip_if_numa_unavailable()) {
    return;
  }

  std::pair<std::string, bool> result = invoke_normalize_cpu_string("");
  ASSERT_TRUE(result.second);
  ASSERT_EQ("", result.first);

  result = invoke_normalize_cpu_string("  ");
  ASSERT_TRUE(result.second);
  ASSERT_EQ("", result.first);

  result = invoke_normalize_cpu_string("1-5,6,7");
  ASSERT_TRUE(result.second);
  ASSERT_EQ("1-5,6,7", result.first);

  result = invoke_normalize_cpu_string("01-2");
  ASSERT_TRUE(result.second);
  ASSERT_EQ("1-2", result.first);

  result = invoke_normalize_cpu_string("1-02");
  ASSERT_TRUE(result.second);
  ASSERT_EQ("1-2", result.first);

  result = invoke_normalize_cpu_string(" 1 - 5 ,6,7 ");
  ASSERT_TRUE(result.second);
  ASSERT_EQ("1-5,6,7", result.first);

  result = invoke_normalize_cpu_string("1-5,?6,7");
  ASSERT_FALSE(result.second);
}

TEST_F(SchedAffinityManagerTest, InitSchedAffinityInfo) {
  if (skip_if_numa_unavailable()) {
    return;
  }

  auto instance =
      Sched_affinity_manager::create_instance(default_config, false);
  auto true_type_instance =
      dynamic_cast<Sched_affinity_manager_numa *>(instance);
  bitmask *bitmask = INVALID_BITMASK_PTR;

  bool result = SchedAffinityManagerTest::invoke_init_sched_affinity_info(
      *true_type_instance, "", bitmask);
  ASSERT_TRUE(result);
  ASSERT_EQ(bitmask, nullptr);
  bitmask = INVALID_BITMASK_PTR;

  result = SchedAffinityManagerTest::invoke_init_sched_affinity_info(
      *true_type_instance, "1-3", bitmask);
  ASSERT_TRUE(result);
  ASSERT_NE(bitmask, nullptr);
  auto expected = numa_parse_cpustring("1-3");
  ASSERT_EQ(1, numa_bitmask_equal(expected, bitmask));
  numa_free_cpumask(expected);
  numa_free_cpumask(bitmask);
  bitmask = INVALID_BITMASK_PTR;

  result = SchedAffinityManagerTest::invoke_init_sched_affinity_info(
      *true_type_instance, "??", bitmask);
  ASSERT_FALSE(result);
  ASSERT_EQ(bitmask, nullptr);
  bitmask = INVALID_BITMASK_PTR;

  Sched_affinity_manager::free_instance();
}

TEST_F(SchedAffinityManagerTest, InitSchedAffinityGroup) {
  if (skip_if_numa_unavailable()) {
    return;
  }

  auto instance =
      Sched_affinity_manager::create_instance(default_config, false);
  auto true_type_instance =
      dynamic_cast<Sched_affinity_manager_numa *>(instance);

  bitmask *bitmask = INVALID_BITMASK_PTR;
  std::vector<Sched_affinity_group> sched_affinity_groups;

  SchedAffinityManagerTest::invoke_init_sched_affinity_info(*true_type_instance,
                                                            "1-3", bitmask);

  bool result = SchedAffinityManagerTest::invoke_init_sched_affinity_group(
      *true_type_instance, bitmask, false, sched_affinity_groups);
  ASSERT_TRUE(result);
  ASSERT_EQ(1, sched_affinity_groups.size());
  ASSERT_EQ(
      1, numa_bitmask_equal(bitmask, sched_affinity_groups[0].avail_cpu_mask));
  ASSERT_EQ(0, sched_affinity_groups[0].assigned_thread_num);
  ASSERT_EQ(3, sched_affinity_groups[0].avail_cpu_num);

  sched_affinity_groups.clear();
  result = SchedAffinityManagerTest::invoke_init_sched_affinity_group(
      *true_type_instance, bitmask, true, sched_affinity_groups);
  ASSERT_TRUE(result);
  ASSERT_EQ(4, sched_affinity_groups.size());
  ASSERT_TRUE(
      std::all_of(sched_affinity_groups.begin(), sched_affinity_groups.end(),
                  [](Sched_affinity_group &sched_affinity_group) {
                    return sched_affinity_group.assigned_thread_num == 0;
                  }));
  ASSERT_EQ(3, std::accumulate(
                   sched_affinity_groups.begin(), sched_affinity_groups.end(),
                   0, [](int v, Sched_affinity_group &sched_affinity_group) {
                     return v + sched_affinity_group.avail_cpu_num;
                   }));

  numa_free_cpumask(bitmask);

  Sched_affinity_manager::free_instance();
}

TEST_F(SchedAffinityManagerTest, IsThreadSchedEnabled) {
  using namespace std::chrono_literals;

  if (skip_if_numa_unavailable()) {
    return;
  }

  std::map<sched_affinity::Thread_type, const char *> config = {
      {sched_affinity::Thread_type::FOREGROUND, "1-3"}};

  auto instance = Sched_affinity_manager::create_instance(config, false);
  auto true_type_instance =
      dynamic_cast<Sched_affinity_manager_numa *>(instance);

  for (auto thread_type : thread_types) {
    if (thread_type == Thread_type::FOREGROUND) {
      ASSERT_TRUE(
          invoke_is_thread_sched_enabled(*true_type_instance, thread_type));
    } else {
      ASSERT_FALSE(
          invoke_is_thread_sched_enabled(*true_type_instance, thread_type));
    }
  }

  Sched_affinity_manager::free_instance();
}

TEST_F(SchedAffinityManagerTest, BindToGroup) {
  using namespace std::chrono_literals;

  if (skip_if_numa_unavailable()) {
    return;
  }

  std::map<sched_affinity::Thread_type, const char *> config = {
      {sched_affinity::Thread_type::FOREGROUND, "1-3"}};

  auto instance = Sched_affinity_manager::create_instance(config, false);
  auto true_type_instance =
      dynamic_cast<Sched_affinity_manager_numa *>(instance);

  std::map<Thread_type, std::set<pid_t>> &m_thread_pid =
      get_m_thread_pid(*true_type_instance);

  std::atomic<bool> is_stopped;
  std::vector<pid_t> thread_pids;
  std::vector<std::thread> threads;
  start_threads(1, threads, thread_pids, is_stopped);

  auto pid = thread_pids[0];
  m_thread_pid[Thread_type::FOREGROUND].insert(pid);

  bool result =
      SchedAffinityManagerTest::invoke_bind_to_group(*true_type_instance, pid);
  ASSERT_TRUE(result);
  std::map<Thread_type, std::vector<Sched_affinity_group>> &
      m_sched_affinity_groups = get_m_sched_affinity_group(*true_type_instance);
  auto sched_affinity_groups =
      m_sched_affinity_groups.at(Thread_type::FOREGROUND);
  bitmask *bitmask = numa_allocate_cpumask();
  numa_sched_getaffinity(pid, bitmask);
  std::map<pid_t, int> &pid_group_id =
      SchedAffinityManagerTest::get_m_pid_group_id(*true_type_instance);
  ASSERT_NE(pid_group_id.end(), pid_group_id.find(pid));
  auto group_id = pid_group_id.at(pid);
  ASSERT_TRUE(group_id >= 0 &&
              group_id < static_cast<int>(sched_affinity_groups.size()));
  ASSERT_GE(sched_affinity_groups[group_id].avail_cpu_num, 1);
  ASSERT_EQ(1, sched_affinity_groups[group_id].assigned_thread_num);
  ASSERT_EQ(1, numa_bitmask_equal(
                   sched_affinity_groups[group_id].avail_cpu_mask, bitmask));

  is_stopped.store(true);
  std::for_each(threads.begin(), threads.end(),
                [](std::thread &thread) { thread.join(); });

  numa_free_cpumask(bitmask);
  Sched_affinity_manager::free_instance();
}

TEST_F(SchedAffinityManagerTest, UnbindFromGroup) {
  using namespace std::chrono_literals;

  if (skip_if_numa_unavailable()) {
    return;
  }

  std::map<sched_affinity::Thread_type, const char *> config = {
      {sched_affinity::Thread_type::FOREGROUND, "1-3"}};

  auto instance = Sched_affinity_manager::create_instance(config, false);
  auto true_type_instance =
      dynamic_cast<Sched_affinity_manager_numa *>(instance);

  std::map<Thread_type, std::set<pid_t>> &m_thread_pid =
      get_m_thread_pid(*true_type_instance);

  std::atomic<bool> is_stopped;
  std::vector<pid_t> thread_pids;
  std::vector<std::thread> threads;
  start_threads(1, threads, thread_pids, is_stopped);

  auto pid = thread_pids[0];
  m_thread_pid[Thread_type::FOREGROUND].insert(pid);

  SchedAffinityManagerTest::invoke_bind_to_group(*true_type_instance, pid);
  std::map<Thread_type, std::vector<Sched_affinity_group>> &
      m_sched_affinity_groups = get_m_sched_affinity_group(*true_type_instance);
  auto &sched_affinity_groups =
      m_sched_affinity_groups.at(Thread_type::FOREGROUND);
  std::map<pid_t, int> &pid_group_id =
      SchedAffinityManagerTest::get_m_pid_group_id(*true_type_instance);

  ASSERT_FALSE(pid_group_id.empty());
  bool result = SchedAffinityManagerTest::invoke_unbind_from_group(
      *true_type_instance, pid);
  ASSERT_TRUE(result);
  ASSERT_TRUE(pid_group_id.empty());
  for (auto &group : sched_affinity_groups) {
    ASSERT_EQ(0, group.assigned_thread_num);
  }

  is_stopped.store(true);
  std::for_each(threads.begin(), threads.end(),
                [](std::thread &thread) { thread.join(); });

  Sched_affinity_manager::free_instance();
}

TEST_F(SchedAffinityManagerTest, GetThreadTypeByPid) {
  if (skip_if_numa_unavailable()) {
    return;
  }

  std::map<sched_affinity::Thread_type, const char *> config = {
      {sched_affinity::Thread_type::FOREGROUND, "1-3"}};

  auto instance = Sched_affinity_manager::create_instance(config, false);
  auto true_type_instance =
      dynamic_cast<Sched_affinity_manager_numa *>(instance);

  std::map<Thread_type, std::set<pid_t>> &m_thread_pid =
      get_m_thread_pid(*true_type_instance);

  auto pid = gettid();
  for (auto thread_type : thread_types) {
    ASSERT_TRUE(m_thread_pid[thread_type].empty());
    m_thread_pid[thread_type].insert(pid);
    ASSERT_EQ(thread_type,
              SchedAffinityManagerTest::invoke_get_thread_type_by_pid(
                  *true_type_instance, pid));
    m_thread_pid[thread_type].erase(pid);
    ASSERT_TRUE(m_thread_pid[thread_type].empty());
  }

  Sched_affinity_manager::free_instance();
}

TEST_F(SchedAffinityManagerTest, CheckCpuString) {
  if (skip_if_numa_unavailable()) {
    return;
  }
  auto instance = Sched_affinity_manager::create_instance(default_config, false);
  ASSERT_TRUE(instance->check_cpu_string(""));
  ASSERT_TRUE(instance->check_cpu_string(" "));
  ASSERT_TRUE(instance->check_cpu_string("  "));
  ASSERT_TRUE(instance->check_cpu_string("1"));
  ASSERT_TRUE(instance->check_cpu_string("1,2"));
  ASSERT_TRUE(instance->check_cpu_string("1-3"));
  ASSERT_TRUE(instance->check_cpu_string("1,2,3-4"));
  ASSERT_TRUE(instance->check_cpu_string(" 1  ,2, 3- 4 "));
  ASSERT_FALSE(instance->check_cpu_string(",,2, 3- 4 "));
  ASSERT_FALSE(instance->check_cpu_string("3--4"));
  ASSERT_FALSE(instance->check_cpu_string("3,,4"));
  ASSERT_FALSE(instance->check_cpu_string("3,4,"));
  ASSERT_FALSE(instance->check_cpu_string("3,4,,"));
  ASSERT_FALSE(instance->check_cpu_string(",3,4"));
  ASSERT_FALSE(instance->check_cpu_string("3-4?"));
  ASSERT_FALSE(instance->check_cpu_string("3-?4"));
  ASSERT_FALSE(instance->check_cpu_string("3?4"));
  ASSERT_FALSE(instance->check_cpu_string("?"));

  Sched_affinity_manager::free_instance();
}

TEST_F(SchedAffinityManagerTest, RegisterThread) {
  if (skip_if_numa_unavailable()) {
    return;
  }

  std::map<sched_affinity::Thread_type, const char *> config = {
      {sched_affinity::Thread_type::FOREGROUND, "1-3"}};

  auto instance = Sched_affinity_manager::create_instance(config, false);
  auto true_type_instance =
      dynamic_cast<Sched_affinity_manager_numa *>(instance);

  std::atomic<bool> is_stopped;
  std::vector<pid_t> thread_pids;
  std::vector<std::thread> threads;
  int thread_number = 5;
  start_threads(thread_number, threads, thread_pids, is_stopped);

  std::map<Thread_type, std::set<pid_t>> &m_thread_pid =
      get_m_thread_pid(*true_type_instance);

  std::map<pid_t, int> &pid_group_id =
      SchedAffinityManagerTest::get_m_pid_group_id(*true_type_instance);

  for (auto &thread_pid : thread_pids) {
    instance->register_thread(Thread_type::FOREGROUND, thread_pid);
  }

  ASSERT_EQ(thread_number, m_thread_pid[Thread_type::FOREGROUND].size());
  ASSERT_EQ(thread_number, pid_group_id.size());
  ASSERT_TRUE(std::all_of(pid_group_id.begin(), pid_group_id.end(),
                          [](auto &kv) { return kv.second >= 0; }));

  is_stopped.store(true);
  std::for_each(threads.begin(), threads.end(),
                [](std::thread &thread) { thread.join(); });

  Sched_affinity_manager::free_instance();
}

TEST_F(SchedAffinityManagerTest, UnregisterThread) {
  if (skip_if_numa_unavailable()) {
    return;
  }

  std::map<sched_affinity::Thread_type, const char *> config = {
      {sched_affinity::Thread_type::FOREGROUND, "1-3"}};

  auto instance = Sched_affinity_manager::create_instance(config, false);
  auto true_type_instance =
      dynamic_cast<Sched_affinity_manager_numa *>(instance);

  std::atomic<bool> is_stopped;
  std::vector<pid_t> thread_pids;
  std::vector<std::thread> threads;
  int thread_number = 5;
  start_threads(thread_number, threads, thread_pids, is_stopped);

  std::map<Thread_type, std::set<pid_t>> &m_thread_pid =
      get_m_thread_pid(*true_type_instance);

  std::map<pid_t, int> &pid_group_id =
      SchedAffinityManagerTest::get_m_pid_group_id(*true_type_instance);

  for (auto &thread_pid : thread_pids) {
    instance->register_thread(Thread_type::FOREGROUND, thread_pid);
  }

  for (auto &thread_pid : thread_pids) {
    instance->unregister_thread(Thread_type::FOREGROUND, thread_pid);
  }

  ASSERT_EQ(0, m_thread_pid[Thread_type::FOREGROUND].size());
  ASSERT_EQ(0, pid_group_id.size());

  is_stopped.store(true);
  std::for_each(threads.begin(), threads.end(),
                [](std::thread &thread) { thread.join(); });

  Sched_affinity_manager::free_instance();
}

TEST_F(SchedAffinityManagerTest, NumaAwareDisabled) {
  if (skip_if_numa_unavailable()) {
    return;
  }

  ASSERT_GT(test_process_node_num, 1);

  int cpu0_index = 0;
  int cpu1_index = cpu_num_per_node;
  std::string cpu_string =
      std::to_string(cpu0_index) + "," + std::to_string(cpu1_index);

  std::map<sched_affinity::Thread_type, const char *> config = {
      {sched_affinity::Thread_type::FOREGROUND, cpu_string.c_str()}};

  auto instance = Sched_affinity_manager::create_instance(config, false);
  auto true_type_instance =
      dynamic_cast<Sched_affinity_manager_numa *>(instance);

  std::map<Thread_type, std::vector<Sched_affinity_group>> &
      m_sched_affinity_groups = get_m_sched_affinity_group(*true_type_instance);

  std::atomic<bool> is_stopped;
  std::vector<pid_t> thread_pids;
  std::vector<std::thread> threads;
  int thread_number = 5;
  start_threads(thread_number, threads, thread_pids, is_stopped);

  for (auto &thread_pid : thread_pids) {
    instance->register_thread(Thread_type::FOREGROUND, thread_pid);
  }

  ASSERT_EQ(1, m_sched_affinity_groups[Thread_type::FOREGROUND].size());
  ASSERT_EQ(2,
            m_sched_affinity_groups[Thread_type::FOREGROUND][0].avail_cpu_num);
  ASSERT_EQ(
      thread_number,
      m_sched_affinity_groups[Thread_type::FOREGROUND][0].assigned_thread_num);

  is_stopped.store(true);
  std::for_each(threads.begin(), threads.end(),
                [](std::thread &thread) { thread.join(); });
  Sched_affinity_manager::free_instance();
}

TEST_F(SchedAffinityManagerTest, NumaAwareEnabled) {
  if (skip_if_numa_unavailable()) {
    return;
  }

  ASSERT_GT(test_process_node_num, 1);

  int cpu0_index = 0;
  int cpu1_index = cpu_num_per_node;
  std::string cpu_string =
      std::to_string(cpu0_index) + "," + std::to_string(cpu0_index + 1) + "," +
      std::to_string(cpu1_index) + "," + std::to_string(cpu1_index + 1);

  std::map<sched_affinity::Thread_type, const char *> config = {
      {sched_affinity::Thread_type::FOREGROUND, cpu_string.c_str()}};

  auto instance = Sched_affinity_manager::create_instance(config, true);
  auto true_type_instance =
      dynamic_cast<Sched_affinity_manager_numa *>(instance);

  std::map<Thread_type, std::vector<Sched_affinity_group>> &
      m_sched_affinity_groups = get_m_sched_affinity_group(*true_type_instance);

  std::atomic<bool> is_stopped;
  std::vector<pid_t> thread_pids;
  std::vector<std::thread> threads;
  int thread_number = 8;
  start_threads(thread_number, threads, thread_pids, is_stopped);

  for (auto &thread_pid : thread_pids) {
    instance->register_thread(Thread_type::FOREGROUND, thread_pid);
  }

  ASSERT_EQ(test_process_node_num,
            m_sched_affinity_groups[Thread_type::FOREGROUND].size());
  ASSERT_EQ(2,
            m_sched_affinity_groups[Thread_type::FOREGROUND][0].avail_cpu_num);
  ASSERT_EQ(2,
            m_sched_affinity_groups[Thread_type::FOREGROUND][1].avail_cpu_num);
  ASSERT_EQ(
      thread_number / 2,
      m_sched_affinity_groups[Thread_type::FOREGROUND][0].assigned_thread_num);
  ASSERT_EQ(
      thread_number / 2,
      m_sched_affinity_groups[Thread_type::FOREGROUND][1].assigned_thread_num);

  is_stopped.store(true);
  std::for_each(threads.begin(), threads.end(),
                [](std::thread &thread) { thread.join(); });
  Sched_affinity_manager::free_instance();
}

TEST_F(SchedAffinityManagerTest, RebalanceGroup) {
  if (skip_if_numa_unavailable()) {
    return;
  }

  ASSERT_GT(test_process_node_num, 1);

  int cpu0_index = 0;
  int cpu1_index = cpu_num_per_node;
  std::string cpu_string =
      std::to_string(cpu0_index) + "," + std::to_string(cpu0_index + 1) + "," +
      std::to_string(cpu1_index) + "," + std::to_string(cpu1_index + 1);

  std::map<sched_affinity::Thread_type, const char *> config = {
      {sched_affinity::Thread_type::FOREGROUND, cpu_string.c_str()}};

  auto instance = Sched_affinity_manager::create_instance(config, true);
  auto true_type_instance =
      dynamic_cast<Sched_affinity_manager_numa *>(instance);

  std::map<Thread_type, std::vector<Sched_affinity_group>> &
      m_sched_affinity_groups = get_m_sched_affinity_group(*true_type_instance);

  std::atomic<bool> is_stopped;
  std::vector<pid_t> thread_pids;
  std::vector<std::thread> threads;
  int thread_number = 8;
  start_threads(thread_number, threads, thread_pids, is_stopped);

  for (auto &thread_pid : thread_pids) {
    instance->register_thread(Thread_type::FOREGROUND, thread_pid);
  }

  std::string new_cpu_string = std::to_string(cpu0_index) + "," +
                               std::to_string(cpu1_index) + "," +
                               std::to_string(cpu1_index + 2);
  instance->rebalance_group(new_cpu_string.c_str(), Thread_type::FOREGROUND,
                            true);

  // TODO this test is expected to fail until rebalance_group is implemented.

  ASSERT_EQ(test_process_node_num, m_sched_affinity_groups[Thread_type::FOREGROUND].size());
  ASSERT_EQ(1,
            m_sched_affinity_groups[Thread_type::FOREGROUND][0].avail_cpu_num);
  ASSERT_EQ(3,
            m_sched_affinity_groups[Thread_type::FOREGROUND][1].avail_cpu_num);
  ASSERT_EQ(
      2,
      m_sched_affinity_groups[Thread_type::FOREGROUND][0].assigned_thread_num);
  ASSERT_EQ(
      6,
      m_sched_affinity_groups[Thread_type::FOREGROUND][1].assigned_thread_num);

  new_cpu_string =
      std::to_string(cpu0_index) + "-" + std::to_string(cpu0_index + 3);
  instance->rebalance_group(new_cpu_string.c_str(), Thread_type::FOREGROUND,
                            true);

  ASSERT_EQ(test_process_node_num, m_sched_affinity_groups[Thread_type::FOREGROUND].size());
  ASSERT_EQ(4,
            m_sched_affinity_groups[Thread_type::FOREGROUND][0].avail_cpu_num);
  ASSERT_EQ(0,
            m_sched_affinity_groups[Thread_type::FOREGROUND][1].avail_cpu_num);
  ASSERT_EQ(
      8,
      m_sched_affinity_groups[Thread_type::FOREGROUND][0].assigned_thread_num);
  ASSERT_EQ(
      0,
      m_sched_affinity_groups[Thread_type::FOREGROUND][1].assigned_thread_num);

  new_cpu_string = "";
  instance->rebalance_group(new_cpu_string.c_str(), Thread_type::FOREGROUND,
                            true);

  ASSERT_EQ(1, m_sched_affinity_groups[Thread_type::FOREGROUND].size());
  ASSERT_EQ(test_process_cpu_num,
            m_sched_affinity_groups[Thread_type::FOREGROUND][0].avail_cpu_num);
  ASSERT_EQ(
      8,
      m_sched_affinity_groups[Thread_type::FOREGROUND][0].assigned_thread_num);

  is_stopped.store(true);
  std::for_each(threads.begin(), threads.end(),
                [](std::thread &thread) { thread.join(); });
  Sched_affinity_manager::free_instance();
}

} // namespace sched_affinity

#endif /* HAVE_LIBNUMA */